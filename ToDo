Need to update database to store cloud data like parent id and file id so we can update easier and quicker (otherwise we have to iterate through the cloud to find files over and over again)
Will also help with the detection of deletion of files in the cloud to delete them locally, (add options later to keep locally)

Need to figure out how to handle the rearrangement of files within the local file store,
and cloud store else we will get duplicate items (again having specific file ID's will help greatly)

Todo:
 new function that checks the database for deleted files
    - if the file is in the db but no longer exists in the stored path three things may have happened - [DONE]
        - moved (if moved just delete and re upload in new place?) - [DONE] (Same as rename)
        - renamed   (tricky as our current methods (path and id) wil not work, perhaps the md5hash,
                    and hope that no one changes a file and renames it withing a sync loop?
                    Maybe running the local sync on a thread may help but this will lead to concurrency issue
                    between the cloud and local) - [DONE] (Just deleting and re uploading)
        - deleted (just delete obvs) - Most important to implement currently - [DONE]
    - we need to figure our what and change the cloud accordingly - [DONE]
    - Need to removed files and folders locally (pretty simple just need to implement proper code for dirs and files) - [DONE]


    - figure out a more permanent solution for stopping files that have just been deleted or renamed being re downloaded
        from the cloud (currently we only sync every 10 seconds(will be longer on relase)), but if you change or delete
        a file during the cloud sync it will download the original file - solution is to check if its in the database
        instead of check if it exists - probably not possible may need to implement solution below

        - Add database support to keep records till a file is deleted from both cloud and local, once both are deleted
          the record can be deleted (Will mean items wont update as quickly but should stop the download problem), then
          when cloud syncing we check if a file is in the databse and check if has been delted locally, if so don't dl
          and deleted eventually and visa versa - doesnt work because the database is not updated whilst were cloud syncing - [FAIL]
          - but we should be able to add to queue of downloads then download if the file is not deleted
            (if file is missing and database says file is deleted (the flag system from before)) then dont download
          - (26/8/2016) - implemented the database deletion structure, need to add the download queue system

        - Maybe another method is to download the metadata in one go instead of recursivly to decrease the time spent on the cloud (less chance of redownload doesnt fix)

    - add support for inserting folders from local into drive (folder upload) - [DONE]

    mb create a drive util class to handle the drive functions in a more concise manner
